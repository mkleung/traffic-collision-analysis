# -*- coding: utf-8 -*-
"""Cind 820 Initial Results and Code .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V27AI0Lv4x2IYcHiwxt2i0UyP7VXgo7V

# CIND 820 Traffic Collision Data Analysis and Prediction
### Initial Results and Code



---


**Table of Contents**

1.   Data Import and Description
2.   Data Preprocessing
3.   Exploratory Data Analysis
4.   Feature Engineering and Algorithm Exploration
5.   Hotspot Visualization using Apriori
6.   Collision Prediction using Logistic Regression
7.   Collision Prediction using Random Forest
8.   Decision Tree
8.   Comparison and Results
8.   Conclusion


---


**Questions**

These are some of the questions we will be investigating during this project

*  What are the locations where the most frequent accidents occur?
*  Do weather and environmental conditions affect traffic collisions?
*  Do traffic measures decrease traffic collisions?


---


**Dataset**

The data is available from Open Ottawa which is a data portal that is freely available to the general public. It consists of traffic collision data from 2017 to 2022 and it is being updated regularly.

It has an Open Data Licence version 2.0 which grants a wordwide, royalty-free, perpetual, non-exclusive licence to copy, modify, publish, translate, adapt and distribute.

https://open.ottawa.ca/datasets/ottawa::traffic-collision-data/about
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f

# Commented out IPython magic to ensure Python compatibility.
# # Packages to install
# # mlxtend for apriori
# # Y-data-profiling for eda report
# 
# %%capture
# 
# !pip install pandas mlxtend -q
# !pip install ydata-profiling -q
# !pip install pandas folium -q
# !pip install scikit-learn --upgrade -q

"""# Section 1. Data Import and Description

---

1.   Import libraries and load the dataset
2.   Display dataset
3.   Print a summary of the data attributes
4.   Show the descriptive statistics for all columns
5.   Check for any missing values



"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Import Libraries and load the dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
# %matplotlib inline

pd.pandas.set_option('display.max_columns', None)
data=pd.read_csv("Traffic_Collision_Data.csv")

# Step 2: Display dataset

# Problematic columns:
# The column Accident_Time contains "Unknown" values
# Accident Year does not contain any data
# X, Y, GeoID, X_Cordinate, Y_Coordinate are duplicates
# ID, GEOID are metadata
# Classification_Of_Accident, etc contains numbers corresponding categorical data

data.head(10)

# Step 3: Print a summary of the data attributes
data.info()

# Step 4: Show the descriptive statistics
print(data.describe(include='all'))

# Step 5: Check for missing values
print("Missing Values:")
print(data.isnull().sum())

"""# Section 2. Data Preprocessing

---
1.  Clean the Data
2.  Drop redundant columns
3.  Fix missing values and duplicates
4.  Handle Outliers
5.  Standardize Categorical Data
"""

# Step 1: Clean the Data

df = data.copy()

# This fixes "Unknown" values in the Accident_Time column by replacing them with a median time
df['Accident_Time1'] = pd.to_datetime(df['Accident_Time'], errors='coerce', format='%H:%M')
median_time = df['Accident_Time1'].dropna().median()
if pd.notnull(median_time):
    median_time_str = median_time.strftime('%H:%M')

df['Accident_Time'] = df['Accident_Time'].replace("Unknown", median_time_str)
df.head()

# Combine the two time columns
def combine_date_time(row):
    try:
        return pd.to_datetime(f"{row['Accident_Date']} {row['Accident_Time']}")
    except Exception:
        return pd.NaT

df['Accident_Timestamp'] = df.apply(combine_date_time, axis=1)
df.head()

# Clean data inside columns

df['Classification_Of_Accident'] = df['Classification_Of_Accident'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Initial_Impact_Type'] = df['Initial_Impact_Type'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Road_Surface_Condition'] = df['Road_Surface_Condition'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Environment_Condition'] = df['Environment_Condition'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Light'] = df['Light'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Traffic_Control'] = df['Traffic_Control'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Max_Injury'] = df['Max_Injury'].str.replace(r'^\d+\s*-\s*', '', regex=True)


df.head()

# Step 2: Drop Redundant Columns

# Accident_Year contains only null values, Accident_Date, Accident_time have been combined
# ObjectID, Geo_ID, ID are database primary keys
# X_Coordinate, Y_Coordinate, X , Y are duplicates of long, lat
# Location column is irrelevant

df = df.drop(columns=["Geo_ID", "Accident_Year", "Accident_Date", "Accident_Time", "Accident_Time1", "X", "Y", "ObjectId", "ID", "X_Coordinate", "Y_Coordinate", "Location"])
df.head()

# Step 3: Fix missing Values and duplicates

# These columns contain mostly NA, fill them with zero
columns_to_fill = [
    'Num_of_Bicycles',
    'Num_of_Motorcycles',
    'Max_Injury',
    'Num_of_Injuries',
    'Num_of_Minimal_Injuries',
    'Num_of_Minor_Injuries',
    'Num_of_Major_Injuries',
    'Num_of_Fatal_Injuries'
]
for column in columns_to_fill:
    df[column] = df[column].fillna(0)

# Fill missing values for the columns that have a low amount of missing values
for col in ['Initial_Impact_Type', 'Road_Surface_Condition', 'Environment_Condition', 'Traffic_Control']:
    mode_value = df[col].mode()
    if not mode_value.empty:  # Ensure mode is not empty
        df[col] = df[col].fillna(mode_value[0])


df.head()

print(df.isnull().sum())

# Detect duplicate rows
duplicates = df.duplicated()
print("Duplicates detected:\n", duplicates)

# Step 4: Handle Outliers
# There does not seem to be many outliers

def plot_histograms(df):
    # Get all numerical columns
    numerical_columns = df.select_dtypes(include=['number']).columns

    # Set up the number of plots
    num_cols = len(numerical_columns)
    plt.figure(figsize=(15, 5 * num_cols))

    # Create a histogram for each numerical column
    for i, column in enumerate(numerical_columns):
        plt.subplot(num_cols, 1, i + 1)  # Create a subplot for each histogram
        plt.hist(df[column], bins=30, alpha=0.7, color='blue', edgecolor='black')
        plt.title(f'Histogram of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')

    # Adjust layout
    plt.tight_layout()
    plt.show()


plot_histograms(df)

# Step 5: One Hot Encode on whole data set

columns_to_encode = [
    'Location_Type', 'Classification_Of_Accident', 'Initial_Impact_Type',
    'Road_Surface_Condition', 'Environment_Condition', 'Light',
    'Traffic_Control', 'Max_Injury'
]

# Ensure the columns to be encoded are treated as strings
for col in columns_to_encode:
    df[col] = df[col].astype(str)

# Perform one-hot encoding
encoded_data = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)

# Display the resulting dataset's shape to confirm encoding
print("Original dataset shape:", df.shape)
print("Encoded dataset shape:", encoded_data.shape)
encoded_data.head(100)

"""# Section 3.   Exploratory Data Analysis
---
1.   Generate the YData Profiling Report


"""

# 1. Generate the YData Profiling Report

from ydata_profiling import ProfileReport
profile = ProfileReport(encoded_data, title="Traffic Collision Report")
profile.to_file("collision_report.html")

# Create correlation matrix

numeric_df = encoded_data.select_dtypes(include=['number'])
correlation_matrix = numeric_df.corr()
print(correlation_matrix)

# Generate a HeatMap
plt.figure(figsize=(25,25))
sns.heatmap(correlation_matrix, annot=True)

"""# Section 4. Visualize the hotspots using Apriori
---

"""

df.head()

from mlxtend.frequent_patterns import apriori, association_rules

# Filter relevant columns
data_hotspots = df[['Lat', 'Long', 'Num_of_Injuries', 'Num_of_Fatal_Injuries']]

# Create a unique identifier for each location
data_hotspots['Location'] = data_hotspots['Lat'].astype(str) + "_" + data_hotspots['Long'].astype(str)

# Aggregate data by unique location
hotspot_summary = data_hotspots.groupby('Location').agg({
    'Num_of_Injuries': 'sum',
    'Num_of_Fatal_Injuries': 'sum'
}).reset_index()

# Create a binary presence matrix for hotspots
hotspot_summary['Hotspot'] = (hotspot_summary['Num_of_Injuries'] > 0) | (hotspot_summary['Num_of_Fatal_Injuries'] > 0)

# Convert to boolean format for Apriori
hotspot_matrix = pd.get_dummies(hotspot_summary[['Hotspot']], dtype=int)

# Apply Apriori algorithm
frequent_itemsets = apriori(hotspot_matrix, min_support=0.1, use_colnames=True)

# Generate association rules, providing 'num_itemsets' argument (Assuming 2 or more hotspots are in your dataset)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1, support_only=True)

# Display the results
print("Frequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules:")
print(rules)

import pandas as pd
import folium

def create_map(df, lat_column, lon_column, value_column=None):
    m = folium.Map(location=[df[lat_column].mean(), df[lon_column].mean()], zoom_start=12)

    for _, row in df.iterrows():
      if row[value_column] > 0:
        popup_text = f'{value_column}: {row[value_column]}' if value_column else None
        folium.CircleMarker(
            location=(row[lat_column], row[lon_column]),
            radius=1,
            color='red',
            fill=True,
            fill_color='red',
            fill_opacity=0.5,
            popup=popup_text
        ).add_to(m)

    return m

lat_column = 'Lat'
lon_column = 'Long'
value_column = 'Num_of_Injuries'

# Create the map
map_with_dots = create_map(encoded_data, lat_column, lon_column, value_column)

map_with_dots

# Save the map to an HTML file
# map_with_dots.save('map_with_dots.html')

"""# Section 5. Predictive Analytics using Logistic Regression
---

Logistic Regression
*  Binary outcome (e.g. collision or no collision)


# Feature Selection

*  Latitude, Longitude
*  Accident_Year, Accident_Date, Accident_Time
*  Environmental factors, Road_Surface_Condition, Environment_Condition, Light)
*  Traffic-related features (Traffic_Control, Num_of_Vehicle)

"""

df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix


# Create a binary target variable for collisions
df['Collision'] = df['Num_of_Injuries'] > 0  # True if there are injuries, else False

# Select relevant features
features = df[['Lat', 'Long', 'Num_of_Vehicle', 'Num_Of_Pedestrians', 'Num_of_Bicycles', 'Num_of_Motorcycles', 'Road_Surface_Condition', 'Environment_Condition', 'Light']]

# Encode categorical features
features = pd.get_dummies(features, drop_first=True)

# Define the target variable
target = df['Collision'].astype(int)  # Convert boolean to int (0 or 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# Section 6. Predictive Analytics using Decision Trees

---


"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

df['Collision'] = df['Num_of_Injuries'] > 0  # True if there are injuries, else False

# Check for required columns
required_columns = ['Lat', 'Long', 'Num_of_Vehicle', 'Num_Of_Pedestrians',
                    'Num_of_Bicycles', 'Num_of_Motorcycles',
                    'Road_Surface_Condition', 'Environment_Condition', 'Light']

# Ensure all required columns are present
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print("Missing columns:", missing_columns)
else:
    # Select features
    features = data[required_columns]

    # Encode categorical features
    features = pd.get_dummies(features, drop_first=True)

    # Define the target variable
    target = df['Collision'].astype(int)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

    # Initialize the decision tree classifier
    model = DecisionTreeClassifier(random_state=42)

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

"""# Section 7. Predictive Analytics using Random Forest

---

Feature Engineering
*  target variable

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Create a binary target variable for collisions
df['Collision'] = df['Num_of_Injuries'] > 0  # True if there are injuries, else False

# Check for required columns
required_columns = ['Lat', 'Long', 'Num_of_Vehicle', 'Num_Of_Pedestrians',
                    'Num_of_Bicycles', 'Num_of_Motorcycles',
                    'Road_Surface_Condition', 'Environment_Condition', 'Light']

# Ensure all required columns are present
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    print("Missing columns:", missing_columns)
else:
    # Select features
    features = data[required_columns]

    # Encode categorical features
    features = pd.get_dummies(features, drop_first=True)

    # Define the target variable
    target = df['Collision'].astype(int)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

    # Initialize the random forest classifier
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

"""# Section 9 - Predict Collision based on given lat and long

---

*  User enters a latitutde and longitude and the app returns collision
*  In a future iteration, user can enter a route and the application can display collision probability

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error


# Create a new column for total collisions at each location
df['Total_Collisions'] = df['Num_of_Injuries'] + df['Num_of_Fatal_Injuries']

# Calculate the total number of accidents per location
collision_counts = df.groupby(['Lat', 'Long'])['Total_Collisions'].sum().reset_index()

# Calculate the percentage of collisions per location
# Assuming you have a total number of recorded accidents in the dataset to calculate percentages
total_accidents = df['Total_Collisions'].sum()
collision_counts['Collision_Percentage'] = (collision_counts['Total_Collisions'] / total_accidents) * 100

# Prepare features and target variable
features = collision_counts[['Lat', 'Long']]
target = collision_counts['Collision_Percentage']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Initialize the random forest regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Function to predict collision percentage at a user-defined location
def predict_collision_percentage(lat, long):
    location = pd.DataFrame({'Lat': [lat], 'Long': [long]})
    predicted_percentage = model.predict(location)
    return predicted_percentage[0]

# Example user input
user_lat = float(input("Enter Latitude: "))
user_long = float(input("Enter Longitude: "))

predicted_percentage = predict_collision_percentage(user_lat, user_long)
print(f"Predicted Collision Percentage at ({user_lat}, {user_long}): {predicted_percentage:.2f}%")