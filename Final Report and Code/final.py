# -*- coding: utf-8 -*-
"""Cind_820_Initial_Results_and_Code_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jw9EHoYPR9diK1yNoHLaULs-AXL7mv_P

# CIND 820 Traffic Collision Data Analysis and Prediction

ID: 500866890

### Initial Results and Code



---

**Intro**

This project aims to analyze road accident data from 2017-2023 to gain insights into the factors contributing to accidents, identify patterns, and propose data-driven recommendations for improving road safety. The dataset used in this project contains information about various aspects of road accidents, such as location, time, weather conditions, and severity.


**Github Repo**

https://github.com/mkleung/traffic-collision-analysis



**Table of Contents**

1.   Data Import and Description
2.   Data Preprocessing
3.   Exploratory Data Analysis
5.   Data Analysis using Apriori
6.   Collision Prediction using Decision Tree and Random Forest
7.   Comparison and Results

## Dataset Overview
- **Source:** https://open.ottawa.ca/datasets/ottawa::traffic-collision-data/about
- **Size:** 74,613 rows, 29 columns
- **Date Range:** 2017-2023
- **Format:** CSV
- **Copyright:** Open Data Licence Version 2.0 (worldwide, royalty-free, perpetual, non-exclusive licence to copy, modify, publish, translate, adapt and distribute

## Goal of project
- **Use Case:** Predicting Traffic Collision
- **Industry:** Transportation Safety and Traffic management
-  **Users:** City planners, Law enforcement, General public, Cyclists
- **Result:** Develop an application that can predict collision probability from a user input location or a planned route (tentative).

## Questions

These are some of the questions we will be investigating during this project

*  What are the locations where the most frequent accidents occur?
*  Do weather and environmental conditions affect traffic collisions?
*  Do traffic measures decrease traffic collisions?
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f

# Commented out IPython magic to ensure Python compatibility.
# # Packages to install
# # Y-data-profiling for eda report
# 
# %%capture
# 
# # Create EDA report
# !pip install ydata-profiling -q
# 
# # Display maps
# !pip install pandas folium -q
# 
# # Preprocessing
# !pip install scikit-learn --upgrade -q
#

"""# Section 1. Data Import and Description

---


"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries and load the dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium

# Preproprecessing
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Import clustering algorithms like kmeans and DBSCan
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans

# Import Classification Models
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Import Metrics and Reports
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# %matplotlib inline

pd.pandas.set_option('display.max_columns', None)
data=pd.read_csv("Traffic_Collision_Data.csv")
data.head(10)

# Show the descriptive statistics
data.describe()

"""**Summary of Data Description**

*  Accident_Year has 100% missing values                 
*  These attributes all have >80% missing values:
   * Num_of_Bicycles, Num_of_Motorcycles, Max_Injury, Num_of_Injuries, Num_of_Minimal_Injuries, Num_of_Minor_Injuries, Num_of_Major_Injuries, Num_of_Fatal_Injuries
* These have < 5 missing values
  * Traffic_Control, Initial_Impact_Type, Road_Surface_Condition, environmental condition
*  The column Accident_Time contains some "Unknown" or Null values
*  X, Y, GeoID, X_Cordinate, Y_Coordinate are duplicates and not needed
*  ID, GEOID are metadata
*  Classification_Of_Accident, etc contains numbers corresponding categorical data

# Section 2. Data Cleaning and Preprocessing

---
"""

df = data.copy()

# Drop Redundant Columns

# Accident_Year contains only null values
# ObjectID, Geo_ID, ID are database primary keys
# X_Coordinate, Y_Coordinate, X , Y are duplicates of long, lat
# Location column is irrelevant as lat and long are going to be used in this project
# Num_of_Minimal_Injuries, Num_of_Minor_Injuries, Num_of_Major_Injuries are not required
# as we will use num_of_injuries and fatal injuries
# Location_Type, Classification_Of_Accident both contain only 2 unique values
# which are not descriptive enought for our analysis

columns_to_drop = [
     "Accident_Year",
     "Geo_ID", "ObjectId", "ID",
     "X", "Y", "X_Coordinate", "Y_Coordinate",
     "Location",
     "Num_of_Minimal_Injuries", "Num_of_Minor_Injuries", "Num_of_Major_Injuries",
     "Location_Type", "Classification_Of_Accident", "Initial_Impact_Type"
]
df = df.drop(columns=columns_to_drop)
df.head()

# Remove Unknown values from te Accident_Time and replace with median
# For example, row number 6 has an unknown time

df['Accident_Time1'] = pd.to_datetime(df['Accident_Time'], errors='coerce', format='%H:%M')
median_time = df['Accident_Time1'].dropna().median()
if pd.notnull(median_time):
    median_time_str = median_time.strftime('%H:%M')

df['Accident_Time'] = df['Accident_Time'].replace("Unknown", median_time_str)
df = df.drop(columns=['Accident_Time1'])
df.head(10)

# Combine the date and time into one column
def combine_date_time(row):
    try:
        return pd.to_datetime(f"{row['Accident_Date']} {row['Accident_Time']}")
    except Exception:
        return pd.NaT
df['Accident_Timestamp'] = df.apply(combine_date_time, axis=1)
df = df.drop(columns=['Accident_Date','Accident_Time'])
df.head(1)

# Clean data inside category columns

df['Road_Surface_Condition'] = df['Road_Surface_Condition'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Environment_Condition'] = df['Environment_Condition'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Light'] = df['Light'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Traffic_Control'] = df['Traffic_Control'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df['Max_Injury'] = df['Max_Injury'].str.replace(r'^\d+\s*-\s*', '', regex=True)
df.head(1)

# Remove Null Values

# Replace null values with zero in these columns that have 80% null values
# Context: these null values should be zero because there are no injuries.
columns_to_fill = [
    'Num_of_Bicycles',
    'Num_of_Motorcycles',
    'Num_of_Injuries',
    'Num_of_Fatal_Injuries'
]
for column in columns_to_fill:
    df[column] = df[column].fillna(0)

# Remove the rows that have less than 5 null values
df.dropna(subset=['Road_Surface_Condition'], inplace=True)
df.dropna(subset=['Environment_Condition'], inplace=True)
df.dropna(subset=['Traffic_Control'], inplace=True)

# Max_Injury (Categorical) - null values means no injury sustained will be filled with None
df['Max_Injury'] = df['Max_Injury'].fillna("None")

df.head()

# Check if there are any missing values remaining
print(df.isnull().sum())

# Drop duplicates
Num_of_duplicates = sum(df.duplicated())
df = df.drop_duplicates()
print("Duplicates rows dropped:\n", Num_of_duplicates)

# Standardize column names
# Some column names have missing 's' and wrong capital letters
# Max_Injury is a confusing column title and is renamed to Injury_Type

df.rename(columns={'Num_of_Vehicle': 'Num_of_Vehicles'}, inplace=True)
df.rename(columns={'Num_Of_Pedestrians': 'Num_of_Pedestrians'}, inplace=True)
df.rename(columns={'Max_Injury': 'Injury_Type'}, inplace=True)
df.head(1)

# Detect and Remove Outliers

# Lat contains some outliers because the min is 0
# Num_of_Injuries could have one outlier

# NOTE Z-Score or Interquartile range methods are not required for Lat and Long
# because their outliers are assumed to be user input errors or null values

df.describe()

# Investigate outliers in Num_of_Injuries
# This one value is the one off famous bus crash in 2019

column_name = 'Num_of_Injuries'
temp_injuries = df[df[column_name] > 10]
temp_injuries

# Delete the outlier
row_index = 29688
df = df.drop(index=row_index)
df.reset_index(drop=True, inplace=True)

# Investigate latitude
# There are 130 rows with questionable latitudes which point to somewhere in the UK
column_name = 'Lat'
temp_lat = df[df[column_name] < 40]
temp_lat

# Remove the questionable latitudes
df = df[df['Lat'] >= 40]
df.reset_index(drop=True, inplace=True)

"""**Feature Engineering**"""

# Create a boolean column?

# df['Collision'] = df['Num_of_Injuries'] > 0
# df['Collision'] = df['Collision'].astype(int)
# df.head()

"""**Cardinality**"""

categorical_features = ['Traffic_Control', 'Light', 'Road_Surface_Condition', 'Environment_Condition']
for col in categorical_features:
    print(f"{col}: {df[col].nunique()} unique values out of {len(df)} rows.")

"""**Chi Square Test**"""

# from scipy.stats import chi2_contingency
# for col in categorical_features:
#     contingency_table = pd.crosstab(df[col], df['Collision'])
#     chi2, p, _, _ = chi2_contingency(contingency_table)
#     print(f"{col}: Chi-square p-value = {p:.4f}")

"""**One Hot Encoding**"""

# Step 5: One Hot Encode the categorical variables
columns_to_encode = ['Road_Surface_Condition', 'Environment_Condition', 'Light','Traffic_Control', 'Injury_Type']

# Ensure the columns to be encoded are treated as strings
for col in columns_to_encode:
    df[col] = df[col].astype(str)

# Perform one-hot encoding
encoded_df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)

# Display the resulting dataset's shape to confirm encoding
print("Original dataset shape:", df.shape)
print("Encoded dataset shape:", encoded_df.shape)

unique_counts = encoded_df.nunique()

# Print the count of unique values
for column, count in unique_counts.items():
    print(f"Number of unique values in '{column}': {count}")

"""**Summary of Data preprocessing**

* Numeric Columns that have greater greater than 80% null values have been replaced by zero because they have been likely been omitted during the data entry phase.
* Category Columns that have less than 5 null were removed
* Column titles have been renamed
* Several irrelevant Columns have been dropped
* Duplicates have been removed
* Outliers were found and latitude and removed.
* Categorical columns have been one hot encoded
"""

encoded_df.to_csv('encoded_df.csv', index=False)

df.to_csv('df.csv', index=False)

"""# Section 3.   Exploratory Data Analysis and Visualization
---




"""

# Create histograms to look for any other outliers
encoded_df.hist(bins=60, figsize=(20,10))

# Correlation Heatmap

correlation_matrix = encoded_df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=False, fmt='.2f', cmap='RdBu', square=True)
plt.title('Correlation Heatmap')
plt.show()

# Show highest correlations
correlation_pairs = correlation_matrix.unstack()
sorted_correlations = correlation_pairs.sort_values(ascending=False)
sorted_correlations = sorted_correlations[sorted_correlations < 1]
top_correlations = sorted_correlations.head(10)
print("Top 50 Correlations:")
top_correlations

"""**Anova Test**"""

import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Road_Surface_Condition_Ice
# Traffic_Control_No control
# Environment_Condition_Rain


# Example: ANOVA to see the effect of 'Road_Surface_Condition' on 'Num_of_Injuries'
model = ols('Num_of_Injuries ~ C(Environment_Condition_Rain)', data=encoded_df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

anova_table

"""**Principal Component Analysis**"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Select numerical features for PCA
# Replace with your relevant numerical columns
numerical_features = [
    'Num_of_Vehicles', 'Num_of_Pedestrians', 'Num_of_Bicycles',
    'Num_of_Motorcycles', 'Num_of_Injuries', 'Num_of_Fatal_Injuries',
    # Add other relevant numerical columns
]

# Drop rows with missing values in the selected features
encoded_df_numerical = encoded_df[numerical_features].dropna()

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(encoded_df_numerical)

# Perform PCA
pca = PCA(n_components=2)  # Adjust components as needed
principal_components = pca.fit_transform(scaled_data)

# Create a DataFrame with the principal components
pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Explained variance
explained_variance = pca.explained_variance_ratio_
print(f'Explained variance by each component: {explained_variance}')

# Plot the principal components
plt.figure(figsize=(10, 6))
sns.scatterplot(x=pc_df['PC1'], y=pc_df['PC2'])
plt.title('PCA Result')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid()
plt.show()

"""**Cardinality**"""

from scipy.stats import chi2_contingency

# Cardinality
categorical_features = ['Traffic_Control', 'Light', 'Road_Surface_Condition', 'Environment_Condition']
for col in categorical_features:
    print(f"{col}: {df[col].nunique()} unique values out of {len(df)} rows.")

"""**Visualizations (Histograms and Maps)**"""

# Create accident count by the hour of the day
encoded_df['Hour'] = encoded_df['Accident_Timestamp'].dt.hour
plt.figure(figsize=(10, 6))

# Use encoded_df instead of df to ensure consistency
plt.hist(encoded_df['Hour'], bins=range(0, 25), align='left', color='#0d6efd', edgecolor='#fff')

plt.xticks(range(0, 24))
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Accidents')
plt.title('Counts of Accidents by Hour')
plt.grid(axis='y', alpha=0.75)
plt.show()

"""**Insight**
* The highest accidents occur in the afternoon at around 2pm - 6pm
* Lowest happen at night from 11pm to 6am
"""

# Histogram of accidents happening during the week
encoded_df['Accident_Timestamp'] = pd.to_datetime(encoded_df['Accident_Timestamp'])

# Extract the day of the week (0=Monday, 6=Sunday)
encoded_df['Day_of_Week'] = encoded_df['Accident_Timestamp'].dt.dayofweek

# Create a histogram of accidents by day of the week
plt.figure(figsize=(10, 6))
plt.hist(encoded_df['Day_of_Week'], bins=range(8), align='left', color='#0d6efd', edgecolor='#fff')

# Set the x-ticks to represent days of the week
plt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])

# Add labels and title
plt.xlabel('Day of the Week')
plt.ylabel('Number of Accidents')
plt.title('Counts of Accidents by Day of the Week')

# Show grid and plot
plt.grid(axis='y', alpha=0.75)
plt.show()

"""**Insight**
* A lot of accidents happen during the week with Friday the highest.
* Surprisingly, weekends have low accident rates compared to weekdays
"""

# Function to create histograms
def plot_accident_counts_by_condition(df, column_prefix, title):
    condition_columns = [col for col in df.columns if col.startswith(column_prefix)]
    condition_counts = df[condition_columns].sum().reset_index()
    condition_counts.columns = ['Condition', 'Accident Count']
    condition_counts = condition_counts[condition_counts['Accident Count'] > 0]
    plt.figure(figsize=(12, 6))
    plt.bar(condition_counts['Condition'], condition_counts['Accident Count'], color='#0d6efd', edgecolor='#fff' )
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Condition')
    plt.ylabel('Number of Accidents')
    plt.title(title)
    plt.grid(axis='y', alpha=0.75)
    plt.tight_layout()
    plt.show()

# Show histogram of accident counts per traffic control type
plot_accident_counts_by_condition(encoded_df, 'Traffic_Control_', 'Accident Counts by Traffic Control')

"""
Highest accident counts happen when there is
*   No traffic Control
*   Traffic Signal

"""

# Call the function for road surface conditions
plot_accident_counts_by_condition(encoded_df, 'Road_Surface_Condition_', 'Accident Counts by Road Surface Condition')

"""*   Most accidents occur during wet weather


"""

plot_accident_counts_by_condition(encoded_df, 'Light_', 'Accident Counts by Light Condition')

# Visualize the locations of fatal injuries

def create_map(df, lat_column, lon_column, value_column=None):

    m = folium.Map(location=[df[lat_column].mean(), df[lon_column].mean()], zoom_start=12, zoomControl=False)
    for _, row in df.iterrows():
        if row[value_column] > 0:
            Num_of_Fatal_Injuries = int(row.get("Num_of_Fatal_Injuries", 0)) if pd.notna(row.get("Num_of_Fatal_Injuries")) else 0
            Num_of_Injuries = int(row.get("Num_of_Injuries", 0)) if pd.notna(row.get("Num_of_Injuries")) else 0
            Num_of_Bicycles = int(row.get("Num_of_Bicycles", 0)) if pd.notna(row.get("Num_of_Bicycles")) else 0
            Num_of_Motorcycles = int(row.get("Num_of_Motorcycles", 0)) if pd.notna(row.get("Num_of_Motorcycles")) else 0
            popup_text = (
                f'<div style="width: 200px;">'
                f'<strong>Deaths:</strong> {Num_of_Fatal_Injuries}<br>'
                f'<strong>Injuries:</strong> {Num_of_Injuries}<br>'
                f'<strong>Vehicles:</strong> {row.get("Num_of_Vehicles", "N/A")}<br>'
                f'<strong>Pedestrians:</strong> {row.get("Num_of_Pedestrians", "N/A")}<br>'
                f'<strong>Bicycles:</strong> {Num_of_Bicycles}<br>'
                f'<strong>Motorcyles:</strong> {Num_of_Motorcycles}<br>'
                f'<strong>Time:</strong> {row.get("Accident_Timestamp", "N/A")}'
                f'</div>'
            )

            folium.CircleMarker(
                location=(row[lat_column], row[lon_column]),
                radius=2,
                color='red',
                fill=True,
                fill_color='red',
                fill_opacity=1,
                popup=popup_text
            ).add_to(m)

    return m

lat_column = 'Lat'
lon_column = 'Long'
value_column = 'Num_of_Fatal_Injuries'

map_with_dots = create_map(encoded_df, lat_column, lon_column, value_column)
map_with_dots

encoded_df.head()

"""# Section 4. Classification Models
---


we are interested in knowing if a person will get into a collision based on a specific location

# Logistic Regression
"""

df_lr = df.copy()

# Create a binary target variable for collisions
df_lr['Collision'] = df_lr['Num_of_Injuries'] > 0  # True if there are injuries, else False

# Select relevant features
features = df_lr[['Lat', 'Long', 'Num_of_Vehicles', 'Num_of_Pedestrians', 'Num_of_Bicycles', 'Num_of_Motorcycles', 'Road_Surface_Condition', 'Environment_Condition', 'Light']]

# Encode categorical features
features = pd.get_dummies(features, drop_first=True)

# Define the target variable
target = df_lr['Collision'].astype(int)  # Convert boolean to int (0 or 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Initialize the logistic regression model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

"""# Decision Tree

---

"""

df_dt = df.copy()

# One-hot encoding of categorical variables
df_dt = pd.get_dummies(df_dt, columns=['Road_Surface_Condition', 'Environment_Condition', 'Light', 'Traffic_Control'], drop_first=True, dummy_na=False) # dummy_na=False to prevent creating a new category for NaNs

# Replace 'None' with NaN before converting to numeric
for column in df_dt.select_dtypes(include=['object']).columns:
    df_dt[column] = pd.to_numeric(df_dt[column].replace('None', pd.NA), errors='coerce')

# Impute missing values with the median
# Consider using more appropriate methods for imputation such as KNNImputer
df_dt = df_dt.fillna(df_dt.median())

# Define features and target
features = df_dt.drop(['Num_of_Injuries', 'Accident_Timestamp', 'Lat', 'Long'], axis=1)
target = df_dt['Num_of_Injuries'] > 0  # Binary classification for collision occurrence

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Model Training
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

"""# Random Forest

---


"""

df_rf = df.copy()

# One-hot encoding of categorical variables
df_rf = pd.get_dummies(df_rf, columns=['Road_Surface_Condition', 'Environment_Condition', 'Light', 'Traffic_Control'], drop_first=True)

# Replace 'None' with NaN before converting to numeric
for column in df_rf.select_dtypes(include=['object']).columns:
    df_rf[column] = pd.to_numeric(df_rf[column].replace('None', pd.NA), errors='coerce')

# Impute missing values with the median
df_rf = df_rf.fillna(df_rf.median())

# Define features and target
features = df_rf.drop(['Num_of_Injuries', 'Accident_Timestamp', 'Lat', 'Long'], axis=1)
target = df_rf['Num_of_Injuries'] > 0  # target variable

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Model Training
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

"""# K Nearest Neightbor

---


"""

df_knn = encoded_df.copy()
print(data.isna().sum())
# X = df_knn.drop(columns=['Num_of_Injuries'])
# y = df_knn['Num_of_Injuries']

# Initialize KNN classifier
# knn = KNeighborsClassifier(n_neighbors=5)  #
# Fit the model
# knn.fit(X_train, y_train)

null_counts = df.isnull().sum()
null_counts